{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Sentiment_Analysis_on_Amazon_Reviews.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fluffybird2323/Balance-Pole-on-a-Cart-Reinforcement-Learning/blob/master/Sentiment_Analysis_on_Amazon_Reviews%20with%20Convnets%20and%20GloVe%20Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LutW91yMwRzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01c0a817-b885-4efb-ecbb-e7c47c81608f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "t = Tokenizer(num_words=10000)\n",
        "\n",
        "with open('Book1.csv',encoding = 'ISO-8859-1') as fin:\n",
        "    for line in fin:      \n",
        "        t.fit_on_texts( line.split()) # Fitting the tokenizer line-by-line.\n",
        "\n",
        "M = []\n",
        "\n",
        "with open('Book1.csv',encoding = 'ISO-8859-1') as fin:\n",
        "    for line in fin:\n",
        "        # Converting the lines into matrix, line-by-line.\n",
        "        m = t.texts_to_sequences([line])[0]\n",
        "        M.append(m)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rBP2BnKiwRzO",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRsCcXZjwRzR",
        "colab": {}
      },
      "source": [
        "X = np.array(M)\n",
        "train_size = int(len(X) * .75)\n",
        "X = X[:7134]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KbbyadKWwRzU",
        "outputId": "487a2f58-cc61-4ad5-cb5f-e7ed42e31784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7134,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E2iq8WcfwRze",
        "outputId": "917a1844-2eea-433b-b23f-321a581dcfda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "word_index = t.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(X, maxlen=1000)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11975 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbiKpLYaKiNd",
        "colab_type": "code",
        "outputId": "415deb78-890e-4f6f-f446-3caf9c530d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.50d.txt', encoding='UTF-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9fwpxkTKiNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LugT4714KiNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SBqBjMk2wRzg",
        "outputId": "43228abe-4e26-4f22-9b88-f8aefcc23e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import csv\n",
        "with open('Y.csv') as raw:\n",
        "    stars = np.array(list(csv.reader(raw,delimiter=',')))\n",
        "print(stars.reshape(7134,1))    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['4']\n",
            " ['5']\n",
            " ['5']\n",
            " ...\n",
            " ['5']\n",
            " ['5']\n",
            " ['5']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0qX8bpC7wRzk",
        "colab": {}
      },
      "source": [
        "Y = stars \n",
        "from sklearn.utils import shuffle\n",
        "data,Y = shuffle(data,Y)\n",
        "Xtrain = data[:train_size]\n",
        "Xtest = data[train_size:]\n",
        "Ytrain = Y[:train_size] \n",
        "Ytest = Y[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3cN24NJlwRzm",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.utils import to_categorical\n",
        "Ytrain = to_categorical(Ytrain.astype(np.int) -1) \n",
        "Ytest = to_categorical(Ytest.astype(np.int) -1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zEbISUnzwRzo",
        "outputId": "d35af8f6-b05a-492d-87e3-6843c85fb5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(512, 5, activation='relu',kernel_regularizer=regularizers.l2(0.009))(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='elu',kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "x = Dropout(rate = 0.1)(x)\n",
        "x = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.002))(x)\n",
        "preds = Dense(len(Ytrain.T), activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.metrics_names)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ESK_ZhZ_wRzr",
        "outputId": "559efa0d-bdb4-4dad-8747-5457e51b100f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 1000, 50)          598800    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 996, 512)          128512    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 199, 512)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 101888)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              104334336 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 105,193,493\n",
            "Trainable params: 104,594,693\n",
            "Non-trainable params: 598,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yx_MpWRkwRzu",
        "colab": {}
      },
      "source": [
        "batch_size = 1024\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ea7kj8jEwRzw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3494
        },
        "outputId": "43c1b0de-3407-428c-da2a-bd5fd0ccf8a2"
      },
      "source": [
        "history = model.fit(Xtrain, Ytrain, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(Xtest, Ytest, batch_size=batch_size, verbose=1)\n",
        "score1 = model.evaluate(Xtrain, Ytrain, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Train loss:', score1[0])\n",
        "print('Train accuracy:', score1[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4893 samples, validate on 544 samples\n",
            "Epoch 1/100\n",
            "4893/4893 [==============================] - 5s 924us/step - loss: 1.6851 - acc: 0.7996 - val_loss: 1.5307 - val_acc: 0.8004\n",
            "Epoch 2/100\n",
            "4893/4893 [==============================] - 3s 561us/step - loss: 1.4339 - acc: 0.8008 - val_loss: 1.2920 - val_acc: 0.8004\n",
            "Epoch 3/100\n",
            "4893/4893 [==============================] - 3s 562us/step - loss: 1.2115 - acc: 0.8149 - val_loss: 1.1007 - val_acc: 0.8478\n",
            "Epoch 4/100\n",
            "4893/4893 [==============================] - 3s 568us/step - loss: 1.0519 - acc: 0.8565 - val_loss: 0.9942 - val_acc: 0.8515\n",
            "Epoch 5/100\n",
            "4893/4893 [==============================] - 3s 569us/step - loss: 0.9477 - acc: 0.8628 - val_loss: 0.8998 - val_acc: 0.8603\n",
            "Epoch 6/100\n",
            "4893/4893 [==============================] - 3s 569us/step - loss: 0.8637 - acc: 0.8658 - val_loss: 0.8334 - val_acc: 0.8585\n",
            "Epoch 7/100\n",
            "4893/4893 [==============================] - 3s 574us/step - loss: 0.8014 - acc: 0.8661 - val_loss: 0.7807 - val_acc: 0.8585\n",
            "Epoch 8/100\n",
            "4893/4893 [==============================] - 3s 573us/step - loss: 0.7512 - acc: 0.8688 - val_loss: 0.7383 - val_acc: 0.8566\n",
            "Epoch 9/100\n",
            "4893/4893 [==============================] - 3s 574us/step - loss: 0.7102 - acc: 0.8688 - val_loss: 0.7023 - val_acc: 0.8607\n",
            "Epoch 10/100\n",
            "4893/4893 [==============================] - 3s 577us/step - loss: 0.6770 - acc: 0.8688 - val_loss: 0.6745 - val_acc: 0.8588\n",
            "Epoch 11/100\n",
            "4893/4893 [==============================] - 3s 579us/step - loss: 0.6501 - acc: 0.8697 - val_loss: 0.6502 - val_acc: 0.8607\n",
            "Epoch 12/100\n",
            "4893/4893 [==============================] - 3s 580us/step - loss: 0.6268 - acc: 0.8715 - val_loss: 0.6320 - val_acc: 0.8607\n",
            "Epoch 13/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.6076 - acc: 0.8723 - val_loss: 0.6142 - val_acc: 0.8596\n",
            "Epoch 14/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.5893 - acc: 0.8737 - val_loss: 0.5985 - val_acc: 0.8614\n",
            "Epoch 15/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.5727 - acc: 0.8749 - val_loss: 0.5862 - val_acc: 0.8621\n",
            "Epoch 16/100\n",
            "4893/4893 [==============================] - 3s 590us/step - loss: 0.5597 - acc: 0.8750 - val_loss: 0.5758 - val_acc: 0.8588\n",
            "Epoch 17/100\n",
            "4893/4893 [==============================] - 3s 590us/step - loss: 0.5479 - acc: 0.8758 - val_loss: 0.5640 - val_acc: 0.8621\n",
            "Epoch 18/100\n",
            "4893/4893 [==============================] - 3s 593us/step - loss: 0.5353 - acc: 0.8760 - val_loss: 0.5524 - val_acc: 0.8647\n",
            "Epoch 19/100\n",
            "4893/4893 [==============================] - 3s 596us/step - loss: 0.5242 - acc: 0.8776 - val_loss: 0.5449 - val_acc: 0.8621\n",
            "Epoch 20/100\n",
            "4893/4893 [==============================] - 3s 596us/step - loss: 0.5139 - acc: 0.8774 - val_loss: 0.5404 - val_acc: 0.8596\n",
            "Epoch 21/100\n",
            "4893/4893 [==============================] - 3s 596us/step - loss: 0.5054 - acc: 0.8786 - val_loss: 0.5296 - val_acc: 0.8636\n",
            "Epoch 22/100\n",
            "4893/4893 [==============================] - 3s 594us/step - loss: 0.4950 - acc: 0.8804 - val_loss: 0.5236 - val_acc: 0.8636\n",
            "Epoch 23/100\n",
            "4893/4893 [==============================] - 3s 593us/step - loss: 0.4847 - acc: 0.8824 - val_loss: 0.5193 - val_acc: 0.8632\n",
            "Epoch 24/100\n",
            "4893/4893 [==============================] - 3s 592us/step - loss: 0.4768 - acc: 0.8830 - val_loss: 0.5118 - val_acc: 0.8621\n",
            "Epoch 25/100\n",
            "4893/4893 [==============================] - 3s 590us/step - loss: 0.4699 - acc: 0.8856 - val_loss: 0.5075 - val_acc: 0.8636\n",
            "Epoch 26/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.4633 - acc: 0.8862 - val_loss: 0.5086 - val_acc: 0.8662\n",
            "Epoch 27/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.4538 - acc: 0.8899 - val_loss: 0.5103 - val_acc: 0.8618\n",
            "Epoch 28/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.4419 - acc: 0.8928 - val_loss: 0.4998 - val_acc: 0.8636\n",
            "Epoch 29/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.4348 - acc: 0.8974 - val_loss: 0.5062 - val_acc: 0.8614\n",
            "Epoch 30/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.4346 - acc: 0.8971 - val_loss: 0.5194 - val_acc: 0.8485\n",
            "Epoch 31/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.4341 - acc: 0.8960 - val_loss: 0.4950 - val_acc: 0.8643\n",
            "Epoch 32/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.4175 - acc: 0.9041 - val_loss: 0.4959 - val_acc: 0.8643\n",
            "Epoch 33/100\n",
            "4893/4893 [==============================] - 3s 582us/step - loss: 0.4004 - acc: 0.9122 - val_loss: 0.5098 - val_acc: 0.8596\n",
            "Epoch 34/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.4052 - acc: 0.9158 - val_loss: 0.5056 - val_acc: 0.8636\n",
            "Epoch 35/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.3991 - acc: 0.9123 - val_loss: 0.5100 - val_acc: 0.8658\n",
            "Epoch 36/100\n",
            "4893/4893 [==============================] - 3s 582us/step - loss: 0.3853 - acc: 0.9226 - val_loss: 0.5092 - val_acc: 0.8599\n",
            "Epoch 37/100\n",
            "4893/4893 [==============================] - 3s 582us/step - loss: 0.3632 - acc: 0.9300 - val_loss: 0.5224 - val_acc: 0.8522\n",
            "Epoch 38/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.3612 - acc: 0.9363 - val_loss: 0.5434 - val_acc: 0.8621\n",
            "Epoch 39/100\n",
            "4893/4893 [==============================] - 3s 581us/step - loss: 0.3507 - acc: 0.9375 - val_loss: 0.6221 - val_acc: 0.7978\n",
            "Epoch 40/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.4470 - acc: 0.8944 - val_loss: 0.5434 - val_acc: 0.8537\n",
            "Epoch 41/100\n",
            "4893/4893 [==============================] - 3s 582us/step - loss: 0.3926 - acc: 0.9207 - val_loss: 0.5249 - val_acc: 0.8570\n",
            "Epoch 42/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.3751 - acc: 0.9328 - val_loss: 0.5196 - val_acc: 0.8607\n",
            "Epoch 43/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.3530 - acc: 0.9405 - val_loss: 0.5311 - val_acc: 0.8555\n",
            "Epoch 44/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.3396 - acc: 0.9445 - val_loss: 0.5459 - val_acc: 0.8632\n",
            "Epoch 45/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.3273 - acc: 0.9465 - val_loss: 0.5436 - val_acc: 0.8610\n",
            "Epoch 46/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.3263 - acc: 0.9471 - val_loss: 0.6038 - val_acc: 0.8596\n",
            "Epoch 47/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.3249 - acc: 0.9450 - val_loss: 0.5378 - val_acc: 0.8577\n",
            "Epoch 48/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.3173 - acc: 0.9503 - val_loss: 0.5693 - val_acc: 0.8588\n",
            "Epoch 49/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.3034 - acc: 0.9561 - val_loss: 0.5684 - val_acc: 0.8364\n",
            "Epoch 50/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.3009 - acc: 0.9541 - val_loss: 0.5537 - val_acc: 0.8478\n",
            "Epoch 51/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2972 - acc: 0.9583 - val_loss: 0.5815 - val_acc: 0.8577\n",
            "Epoch 52/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2886 - acc: 0.9606 - val_loss: 0.5708 - val_acc: 0.8526\n",
            "Epoch 53/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.2738 - acc: 0.9649 - val_loss: 0.5739 - val_acc: 0.8592\n",
            "Epoch 54/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.3271 - acc: 0.9433 - val_loss: 0.6711 - val_acc: 0.8614\n",
            "Epoch 55/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.4035 - acc: 0.9142 - val_loss: 0.6383 - val_acc: 0.8485\n",
            "Epoch 56/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.3661 - acc: 0.9290 - val_loss: 0.5345 - val_acc: 0.8658\n",
            "Epoch 57/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.3335 - acc: 0.9369 - val_loss: 0.5375 - val_acc: 0.8412\n",
            "Epoch 58/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.3036 - acc: 0.9507 - val_loss: 0.5578 - val_acc: 0.8438\n",
            "Epoch 59/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.2896 - acc: 0.9568 - val_loss: 0.5751 - val_acc: 0.8452\n",
            "Epoch 60/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.2799 - acc: 0.9606 - val_loss: 0.5463 - val_acc: 0.8471\n",
            "Epoch 61/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2662 - acc: 0.9645 - val_loss: 0.5485 - val_acc: 0.8474\n",
            "Epoch 62/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2591 - acc: 0.9664 - val_loss: 0.5542 - val_acc: 0.8551\n",
            "Epoch 63/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2546 - acc: 0.9681 - val_loss: 0.5597 - val_acc: 0.8507\n",
            "Epoch 64/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2481 - acc: 0.9690 - val_loss: 0.5634 - val_acc: 0.8588\n",
            "Epoch 65/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2437 - acc: 0.9700 - val_loss: 0.5648 - val_acc: 0.8563\n",
            "Epoch 66/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2408 - acc: 0.9701 - val_loss: 0.5743 - val_acc: 0.8559\n",
            "Epoch 67/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2378 - acc: 0.9723 - val_loss: 0.5711 - val_acc: 0.8577\n",
            "Epoch 68/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2300 - acc: 0.9726 - val_loss: 0.5725 - val_acc: 0.8614\n",
            "Epoch 69/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2293 - acc: 0.9728 - val_loss: 0.5910 - val_acc: 0.8570\n",
            "Epoch 70/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2351 - acc: 0.9698 - val_loss: 0.5760 - val_acc: 0.8607\n",
            "Epoch 71/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2296 - acc: 0.9721 - val_loss: 0.5699 - val_acc: 0.8507\n",
            "Epoch 72/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2258 - acc: 0.9719 - val_loss: 0.5716 - val_acc: 0.8500\n",
            "Epoch 73/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2237 - acc: 0.9727 - val_loss: 0.5698 - val_acc: 0.8551\n",
            "Epoch 74/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2195 - acc: 0.9732 - val_loss: 0.5702 - val_acc: 0.8555\n",
            "Epoch 75/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.2149 - acc: 0.9739 - val_loss: 0.5772 - val_acc: 0.8515\n",
            "Epoch 76/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2205 - acc: 0.9742 - val_loss: 0.5867 - val_acc: 0.8555\n",
            "Epoch 77/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2402 - acc: 0.9676 - val_loss: 0.6163 - val_acc: 0.8566\n",
            "Epoch 78/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2476 - acc: 0.9662 - val_loss: 0.5632 - val_acc: 0.8599\n",
            "Epoch 79/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.2310 - acc: 0.9706 - val_loss: 0.5668 - val_acc: 0.8496\n",
            "Epoch 80/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2201 - acc: 0.9727 - val_loss: 0.5652 - val_acc: 0.8551\n",
            "Epoch 81/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2132 - acc: 0.9737 - val_loss: 0.5750 - val_acc: 0.8610\n",
            "Epoch 82/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2078 - acc: 0.9757 - val_loss: 0.5804 - val_acc: 0.8563\n",
            "Epoch 83/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2067 - acc: 0.9761 - val_loss: 0.5727 - val_acc: 0.8603\n",
            "Epoch 84/100\n",
            "4893/4893 [==============================] - 3s 584us/step - loss: 0.2029 - acc: 0.9773 - val_loss: 0.5719 - val_acc: 0.8570\n",
            "Epoch 85/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.2028 - acc: 0.9767 - val_loss: 0.6084 - val_acc: 0.8540\n",
            "Epoch 86/100\n",
            "4893/4893 [==============================] - 3s 583us/step - loss: 0.2032 - acc: 0.9760 - val_loss: 0.5606 - val_acc: 0.8537\n",
            "Epoch 87/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.2009 - acc: 0.9763 - val_loss: 0.5557 - val_acc: 0.8603\n",
            "Epoch 88/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2004 - acc: 0.9772 - val_loss: 0.5699 - val_acc: 0.8577\n",
            "Epoch 89/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.1927 - acc: 0.9785 - val_loss: 0.5651 - val_acc: 0.8581\n",
            "Epoch 90/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.1928 - acc: 0.9786 - val_loss: 0.5637 - val_acc: 0.8570\n",
            "Epoch 91/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.1903 - acc: 0.9782 - val_loss: 0.5761 - val_acc: 0.8493\n",
            "Epoch 92/100\n",
            "4893/4893 [==============================] - 3s 585us/step - loss: 0.1996 - acc: 0.9752 - val_loss: 0.6299 - val_acc: 0.8250\n",
            "Epoch 93/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.2444 - acc: 0.9589 - val_loss: 0.5938 - val_acc: 0.8555\n",
            "Epoch 94/100\n",
            "4893/4893 [==============================] - 3s 586us/step - loss: 0.2232 - acc: 0.9693 - val_loss: 0.5770 - val_acc: 0.8390\n",
            "Epoch 95/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2122 - acc: 0.9733 - val_loss: 0.5630 - val_acc: 0.8574\n",
            "Epoch 96/100\n",
            "4893/4893 [==============================] - 3s 591us/step - loss: 0.2038 - acc: 0.9758 - val_loss: 0.5853 - val_acc: 0.8529\n",
            "Epoch 97/100\n",
            "4893/4893 [==============================] - 3s 588us/step - loss: 0.2051 - acc: 0.9777 - val_loss: 0.5726 - val_acc: 0.8581\n",
            "Epoch 98/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2115 - acc: 0.9777 - val_loss: 0.5959 - val_acc: 0.8474\n",
            "Epoch 99/100\n",
            "4893/4893 [==============================] - 3s 589us/step - loss: 0.2061 - acc: 0.9781 - val_loss: 0.5698 - val_acc: 0.8574\n",
            "Epoch 100/100\n",
            "4893/4893 [==============================] - 3s 587us/step - loss: 0.1927 - acc: 0.9805 - val_loss: 0.5841 - val_acc: 0.8574\n",
            "1697/1697 [==============================] - 1s 436us/step\n",
            "5437/5437 [==============================] - 1s 235us/step\n",
            "Test loss: 0.5991198635480652\n",
            "Test accuracy: 0.8569239873568592\n",
            "Train loss: 0.22893199424917302\n",
            "Train accuracy: 0.9685856307256697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xMeGaCI1bvd",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jv4ZyEoH_WfP",
        "outputId": "8e567423-3a3d-43bb-81df-bd63604ba6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "score = model.evaluate(Xtest, Ytest, batch_size=batch_size, verbose=1)\n",
        "score1 = model.evaluate(Xtrain, Ytrain, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Train loss:', score1[0])\n",
        "print('Train accuracy:', score1[1])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1697/1697 [==============================] - 0s 287us/step\n",
            "5437/5437 [==============================] - 1s 253us/step\n",
            "Test loss: 0.5473201404828919\n",
            "Test accuracy: 0.8588096767786888\n",
            "Train loss: 0.3073698836739326\n",
            "Train accuracy: 0.9494574304141826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-0_Ks0poNNoC",
        "colab": {}
      },
      "source": [
        "model.save('SA.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J9VL3zmKYhye",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kTOfR9dn5UMh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}